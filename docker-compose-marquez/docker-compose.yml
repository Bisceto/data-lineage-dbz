# SPDX-License-Identifier: Apache-2.0

version: "3.8"
services:
  airflow:
    image: bitnami/airflow:2
    ports:
      - "8080:8080"
    env_file:
      - openlineage.env
    environment:
      - AIRFLOW_USERNAME=airflow
      - AIRFLOW_PASSWORD=airflow
      - AIRFLOW_EMAIL=airflow@example.com
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - /whl:/whl
      - type: bind
        # source: ${PWD}/requirements.txt
        source: ./requirements.txt
        target: /bitnami/python/requirements.txt

  airflow_scheduler:
    image: bitnami/airflow-scheduler:2
    env_file:
      - openlineage.env
    environment:
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
      - AIRFLOW_WEBSERVER_HOST=airflow
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - ./whl:/whl
      - type: bind
        source: ./requirements.txt
        target: /bitnami/python/requirements.txt
  airflow_worker:
    build:
      dockerfile: ./docker/airflow-worker-spark/Dockerfile.Airflow-worker
      context: .
    env_file:
      - openlineage.env
    environment:
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
      - AIRFLOW_WEBSERVER_HOST=airflow
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - ${PWD}/whl:/whl
      - type: bind
        source: ./requirements.txt
        target: /bitnami/python/requirements.txt
  marquez:
    image: marquezproject/marquez:latest
    ports:
      - "5000:5000"
      - "5001:5001"
    restart: unless-stopped
    volumes:
      - ./docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    depends_on:
      - postgres
    entrypoint: ["./entrypoint.sh"]
    # Enables SQL statement logging (see: https://www.postgresql.org/docs/12/runtime-config-logging.html#GUC-LOG-STATEMENT)
    # command: ["postgres", "-c", "log_statement=all"]

  marquez_web:
    image: marquezproject/marquez-web:latest
    environment:
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
    ports:
      - "3000:3000"
    stdin_open: true
    tty: true
    depends_on:
      - marquez

  postgres:
    image: bitnami/postgresql:12.1.0
    ports:
      - "5432:5432"
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=
      - AIRFLOW_USER=airflow
      - AIRFLOW_PASSWORD=
      - AIRFLOW_DB=airflow
      - MARQUEZ_USER=marquez
      - MARQUEZ_PASSWORD=
      - MARQUEZ_DB=marquez
      - EXAMPLE_USER=example
      - EXAMPLE_PASSWORD=example
      - EXAMPLE_DB=example
    volumes:
      - ./docker/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
  redis:
    image: bitnami/redis:6.0.6
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
  neo4jServer:
    # Docker image to be used
    image: neo4j:5.7.0-community
    restart: unless-stopped
    ports:
      - 7474:7474
      - 7687:7687
    volumes:
      - ./conf:/conf
      - ./data:/data
      - ./import:/import
      - ./logs:/logs
      - ./plugins:/plugins
    environment:
      # Raise memory limits
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms.memory.heap.initial_size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
  kafka:
    image: "bitnami/kafka:latest"
    restart: unless-stopped
    user: root
    ports:
      - 9092:9092
      - 9094:9094
    volumes:
      - ./kafka:/bitnami
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9094,CONTROLLER://:9093,EXTERNAL://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9094,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_CFG_ADVERTISED_HOST_NAME=localhost
  kafka-connect:
    image: "confluentinc/cp-kafka-connect-base:latest"
    restart: unless-stopped
    ports:
      - 8083:8083
    volumes:
      - ./kafka-connect/connectors:/etc/kafka-connect/jars
    environment:
      - CONNECT_PLUGIN_PATH=/usr/share/java,/etc/kafka-connect/jars
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9094
      - CONNECT_GROUP_ID=quickstart
      - CONNECT_CONFIG_STORAGE_TOPIC=quickstart-config
      - CONNECT_OFFSET_STORAGE_TOPIC=quickstart-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=quickstart-status
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=localhost
      - CONNECT_REST_PORT=8083
      - CONNECT_TOPIC_CREATION_QUICKSTART__OFFSETS_REPLICATION_FACTOR=1
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
    depends_on:
      - kafka
  spark:
    image: docker.io/bitnami/spark:3.2.4
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - 7077:7077
    volumes:
      - ./sparkjobs:/usr/local/spark/app

  spark-worker:
    image: docker.io/bitnami/spark:3.2.4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./sparkjobs:/usr/local/spark/app
    depends_on:
      - spark

  arangodb:
    image: arangodb:latest
    restart: on-failure
    ports:
      - 8530:8529
    environment:
      ARANGO_NO_AUTH: 1

  rest-server:
    image: absaoss/spline-rest-server:latest
    restart: on-failure
    ports:
      - 9000:8080
    environment:
      SPLINE_DATABASE_CONNECTION_URL: "arangodb://arangodb/spline"
      # by default /dev/random is used which may block
      CATALINA_OPTS: "-Dsecurerandom.source=file:/dev/./urandom -Djava.security.egd=file:/dev/./urandom"
    depends_on:
      db-init:
        condition: service_completed_successfully

  db-init:
    image: absaoss/spline-admin:latest
    restart: on-failure
    entrypoint: >
      tini -g -- bash -c "
        until curl --output /dev/null --silent --get --fail http://arangodb:8529/_admin/server/availability
        do
          echo waiting for ArangoDB server to be ready...
          sleep 5
        done
        bash ./entrypoint.sh db-init arangodb://arangodb/spline -s
      "
    depends_on:
      - arangodb

  ui:
    image: absaoss/spline-web-ui:latest
    restart: on-failure
    environment:
      # The consumer URL is used by the web browser
      SPLINE_CONSUMER_URL: "http://${DOCKER_HOST_EXTERNAL:-localhost}:9000/consumer"
      # by default /dev/random is used which may block
      CATALINA_OPTS: "-Dsecurerandom.source=file:/dev/./urandom -Djava.security.egd=file:/dev/./urandom"
    ports:
      - 9090:8080
    depends_on:
      - rest-server
