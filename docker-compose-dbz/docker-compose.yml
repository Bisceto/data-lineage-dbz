# SPDX-License-Identifier: Apache-2.0

version: "3.8"
services:
  airflow:
    image: bitnami/airflow:2
    ports:
      - "8080:8080"
    env_file:
      - ./airflow/openlineage.env
    environment:
      - AIRFLOW_USERNAME=airflow
      - AIRFLOW_PASSWORD=airflow
      - AIRFLOW_EMAIL=airflow@example.com
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
    volumes:
      - ./airflow/dags/demo:/opt/bitnami/airflow/dags
      - ./airflow/airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - type: bind
        source: ./airflow/requirements.txt
        target: /bitnami/python/requirements.txt

  airflow_scheduler:
    image: bitnami/airflow-scheduler:2
    env_file:
      - ./airflow/openlineage.env
    environment:
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
      - AIRFLOW_WEBSERVER_HOST=airflow
    volumes:
      - ./airflow/dags/demo:/opt/bitnami/airflow/dags
      - ./airflow/airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - type: bind
        source: ./airflow/requirements.txt
        target: /bitnami/python/requirements.txt
  airflow_worker:
    build:
      dockerfile: ./docker/airflow-worker-spark/Dockerfile.Airflow-worker
      context: .
    env_file:
      - ./airflow/openlineage.env
    environment:
      - AIRFLOW_FERNET_KEY=Z2uDm0ZL60fXNkEXG8LW99Ki2zf8wkmIltaTz1iQPDU=
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_NAME=airflow
      - AIRFLOW_DATABASE_USERNAME=airflow
      - AIRFLOW_DATABASE_PASSWORD=
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_LOAD_EXAMPLES=no
      - AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@postgres:5432/example
      - AIRFLOW_WEBSERVER_HOST=airflow
    volumes:
      - ./airflow/dags/demo:/opt/bitnami/airflow/dags
      - ./airflow/airflow.cfg:/opt/bitnami/airflow/airflow.cfg
      - ./sparkjobs:/usr/local/spark/app
      - type: bind
        source: ./airflow/requirements.txt
        target: /bitnami/python/requirements.txt
  marquez:
    image: marquezproject/marquez:latest
    ports:
      - "5000:5000"
      - "5001:5001"
    restart: unless-stopped
    volumes:
      - ./docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    depends_on:
      - postgres
    entrypoint: ["./entrypoint.sh"]
    # Enables SQL statement logging (see: https://www.postgresql.org/docs/12/runtime-config-logging.html#GUC-LOG-STATEMENT)
    # command: ["postgres", "-c", "log_statement=all"]

  marquez_web:
    image: marquezproject/marquez-web:latest
    environment:
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
    ports:
      - "3000:3000"
    stdin_open: true
    tty: true
    depends_on:
      - marquez

  postgres:
    image: bitnami/postgresql:12.1.0
    ports:
      - "5432:5432"
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=
      - AIRFLOW_USER=airflow
      - AIRFLOW_PASSWORD=
      - AIRFLOW_DB=airflow
      - MARQUEZ_USER=marquez
      - MARQUEZ_PASSWORD=
      - MARQUEZ_DB=marquez
      - EXAMPLE_USER=example
      - EXAMPLE_PASSWORD=example
      - EXAMPLE_DB=example
    volumes:
      - ./docker/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
      - ./sparkjobs:/var/lib/postgresql/data
  redis:
    image: bitnami/redis:6.0.6
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
  neo4jServer:
    # Docker image to be used
    image: neo4j:5.7.0-community
    restart: unless-stopped
    ports:
      - 7474:7474
      - 7687:7687
    volumes:
      - ./neo4j-server/conf:/conf
      - ./neo4j-server/data:/data
      - ./neo4j-server/import:/import
      - ./neo4j-server/logs:/logs
      - ./neo4j-server/plugins:/plugins
    environment:
      # Raise memory limits
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory.heap.initial_size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
  kafka:
    image: "bitnami/kafka:latest"
    restart: unless-stopped
    user: root
    ports:
      - 9092:9092
      - 9094:9094
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9094,CONTROLLER://:9093,EXTERNAL://:9092,DOCKER_HACK://:19092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9094,EXTERNAL://localhost:9092,DOCKER_HACK://kafka:19092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,DOCKER_HACK:PLAINTEXT
      - KAFKA_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_CFG_ADVERTISED_HOST_NAME=localhost
  kafka-connect:
    image: "confluentinc/cp-kafka-connect-base:latest"
    restart: unless-stopped
    ports:
      - 8083:8083
    volumes:
      - ./kafka-connect/connectors:/etc/kafka-connect/jars
    environment:
      - CONNECT_PLUGIN_PATH=/usr/share/java,/etc/kafka-connect/jars
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9094
      - CONNECT_GROUP_ID=quickstart
      - CONNECT_CONFIG_STORAGE_TOPIC=quickstart-config
      - CONNECT_OFFSET_STORAGE_TOPIC=quickstart-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=quickstart-status
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=localhost
      - CONNECT_REST_PORT=8083
      - CONNECT_TOPIC_CREATION_QUICKSTART__OFFSETS_REPLICATION_FACTOR=1
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
    depends_on:
      - kafka
  spark:
    image: docker.io/bitnami/spark:3.2.4
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_HISTORY_UI_ACLS_ENABLE=true
    ports:
      - 7000:8080
      - 7077:7077
    volumes:
      - ./sparkjobs:/usr/local/spark/app

  spark-worker:
    image: docker.io/bitnami/spark:3.2.4
    ports:
      - 8081:8081
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./sparkjobs:/usr/local/spark/app
    depends_on:
      - spark
  neodash:
    image: docker.io/neo4jlabs/neodash
    ports:
      - 5005:5005
